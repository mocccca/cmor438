📚 What is Supervised Learning?
  
Supervised learning is a type of machine learning where the model learns from labeled data — meaning, each training example comes with both the input and the correct output.

⚙️ How Supervised Learning Works

Collect labeled data: Each data point has input features and a known output.
Split into training and test sets: Usually 70–80% for training, the rest for testing.
Choose a model: Based on the problem and data type.
Train the model: Minimize the difference between predictions and true outputs using a loss function.
Evaluate performance: On the test set using metrics like accuracy, precision, recall (for classification) or RMSE (for regression).

🔀 Two Main Types of Supervised Learning

1. Classification
- Predict a category or class.
- Output is discrete.
Examples: Email spam detection, disease diagnosis, sentiment analysis.
2. Regression
- Predict a continuous value.
- Output is numeric.
Examples: Predicting house prices, temperature, stock values.

🔧 Supervised Learning Algorithms

1. The Perceptron
One of the earliest models for binary classification.
Inspired by neurons in the brain.
Works by calculating a weighted sum of inputs and applying a step function:
y
=
{
1
if 
w
⋅
x
+
b
>
0
0
otherwise
 
y={ 
1
0
​	
  
if w⋅x+b>0
otherwise
​	
 
Only works for linearly separable data.
2. Linear Regression
Used for regression tasks.
Models a linear relationship between input X and output y.
y
=
w
⋅
x
+
b
y=w⋅x+b
Learns weights w and bias b to minimize Mean Squared Error (MSE).
Simple, interpretable, but limited to linear patterns.
3. Logistic Regression
Used for binary classification.
Uses the sigmoid function to squash output to the (0, 1) range:
P
(
y
=
1
∣
x
)
=
1
1
+
e
−
(
w
⋅
x
+
b
)
P(y=1∣x)= 
1+e 
−(w⋅x+b)
 
1
​	
 
Outputs probabilities, making it useful for classification with thresholds.
4. Neural Networks
Generalize the perceptron by using multiple layers (input, hidden, output).
Each node applies an activation function like ReLU, tanh, or sigmoid.
Powerful for both regression and classification, especially in deep learning.
Learns by backpropagation and gradient descent.
Example: Image classification, language modeling, time series prediction.
5. K Nearest Neighbors (KNN)
Instance-based learning: No training phase per se.
Predicts the output based on the majority (for classification) or average (for regression) of the k closest data points (neighbors).
Simple, but computationally expensive for large datasets.
6. Decision Trees
Model decisions as a tree of questions based on features.
Each node splits data based on a feature that maximizes some criteria (e.g., information gain for classification or variance reduction for regression).
Easy to visualize, but prone to overfitting.
7. Random Forests
An ensemble of decision trees (usually trained via bagging).
Combines predictions from multiple trees to improve generalization.
Each tree is trained on a random subset of data and features.
Robust to overfitting and works well on most problems out of the box.
8. Other Ensemble Methods
📌 Boosting (e.g., AdaBoost, Gradient Boosting)

Combines multiple weak learners (like shallow trees) sequentially.
Each model focuses on the errors of the previous one.
Final prediction is a weighted sum of all models.
More prone to overfitting but very accurate when tuned well.
📌 Bagging (Bootstrap Aggregation)

Trains models on different bootstrap samples (randomly drawn with replacement).
Reduces variance and helps prevent overfitting.
Random Forest is a type of bagging with decision trees.
